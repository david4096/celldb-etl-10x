"""
CLI entrypoint to tenx etl commands.

Since the list of featureIds are the same for the entire file, we can take
advantage of this for parallelization.

"""
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals

import argparse

from celldb import client

import subprocess

import h5py
import numpy as np

def get_hfile(filepath):
    """
    Returns an hfile handle for the given filepath.
    :param filepath:
    :return:
    """
    return h5py.File(filepath)

def salt_string(unsalted, salt):
    return ('{}{}'.format(unsalted, salt))

def format_barcode(barcode_string):
    """
    Removes the dashes from sample strings.
    :param barcode_string:
    :return:
    """
    return barcode_string.format(barcode_string.replace('-', ''))

def get_features(hfile, group):
    """
    Returns the list of featureId, featureName tuples from the hfile used for
    populating the Features table and for parallelizing the task of loading
    samples.

    :param hfile:
    :param group:
    :return: list of (featureId, featureName) tuplies
    """
    genes = hfile[group + "/genes"]
    gene_names = hfile[group + "/gene_names"]
    return zip(genes, gene_names)

def extract(hfile, group, offset=0, limit=-1):
    """
    This function extracts a row from the h5 file's group sample-wise that can
    be used to populate a row in celldb.

    :param hfile:
    :param group:
    :param offset: how many rows to skip before emitting values
    :return:
    """
    indptr = hfile[group + "/indptr"]
    indices = hfile[group + "/indices"]
    data = hfile[group + "/data"]
    genes = hfile[group + "/genes"]
    barcodes = hfile[group + "/barcodes"]
    shape = hfile[group + "/shape"]
    rowN = shape[0]
    counter_indptr_size = rowN

    if limit > 0:
        limit_value = limit + offset
    else:
        limit_value = len(barcodes)

    for k in range(offset, limit_value):
        barcode = barcodes[k]
        indices_range = indices[indptr[k]:indptr[k + 1]]
        data_range = data[indptr[k]:indptr[k + 1]]
        # we create a dense representation to eliminate confusion between NA
        # and 0
        values = np.zeros(counter_indptr_size, dtype=int)
        for j in range(0, len(data_range)):
            index = indices_range[j]
            value = data_range[j]
            values[index] = value
        formatted_barcode = format_barcode(barcode)
        row = {
            "sample": formatted_barcode,
            "values": values,
            "genes": genes
        }
        yield row




# if k == 31:
#     to = len(barcodes)
# else:
#     to = (k + 1) * len(barcodes) / 31


def transform(row, salt):
    """
    Accepts a row object as generated by yielding from `extract` and transforms
    it into the arguments needed to use the celldb client
    (sampleId, featureIds, values).

    Use this function to modify sampleIds with a salt if necessary.
    :param row:
    :return:
    """
    sampleId = salt_string(row['sample'], salt)
    # Here, we could realize as a list if the data were denormalized, e.g.
    # there were different genes quantified per sample. Since each sample has
    # the same genes quantified the skip this transformation step by setting
    # a list of featureIds that will be available to all load threads.
    featureIds = []  # list(row['genes'])
    values = row['values']

    return sampleId, featureIds, values


def load(cursor, sampleId, featureIds, values):
    """
    This function loads the data into celldb using the celldb client.

    :param sampleId:
    :param featureIds:
    :param values:
    :return: success or failure per row
    """
    # FIXME return value
    print(sampleId)
    # We'll upsert all the features together and so send the kwarg
    # `upsert_features` to false
    return client.upsert_sample(
        cursor, sampleId, featureIds, values, upsert_features=False)


# def mp_load(args):
#     # sampleId = item.sampleId
#     # featureIds = item.featureIds
#     # values = item.values
#     load(cursor, args[0], args[1], args[2])

# get the extent of the file

# get the available local processes

# divide the file into segments

# upsert samples on each thread outputting log results

# record errors

# print errors

def main(args=None):
    parser = argparse.ArgumentParser(
        description='Add 10xgenomics data to celldb')
    parser.add_argument("host", default="localhost:8765", type=str,
        help="The port and host for a celldb instance.")
    parser.add_argument(
        "input", type=str,
        help="A file path pointing to a 10xgenomics .h5 file.")
    parser.add_argument(
        "group", type=str,
        help="The h5 group in the file (mm10 for demonstration).")
    parser.add_argument(
        "--salt", default="", type=str,
        help="The h5 group in the file (mm10 for demonstration).")
    parser.add_argument(
        "--limit", default=-1, type=int,
        help="The number of samples to attempt to upsert starting from the"
             "offset. -1 for no limit")
    parser.add_argument(
        "--offset", default=0, type=int,
        help="The number of items to skip before attempting upsert.")
    parser.add_argument(
        "--subprocesses", default=0, type=int)

    parsed = parser.parse_args(args)
    host = parsed.host
    hfile = get_hfile(parsed.input)
    group = parsed.group
    offset = parsed.offset
    limit = parsed.limit
    salt = parsed.salt
    subprocesses = parsed.subprocesses

    # FIXME Do not multiprocess this way
    # This is a horrible kludge and a real scalable ETL process should use
    # the map-reduce interface via Spark.
    if subprocesses > 0:
        print("Spawning {} subprocesses for this h5 file.".format(
            subprocesses))
        processes = []
        barcodes = hfile[group + "/barcodes"]
        length = len(barcodes)
        chunk = int(length / subprocesses)
        for i in range(subprocesses - 1):
            offset = int(i * chunk)
            print([
                "celldb-etl-10x",
                host,
                parsed.input,
                group,
                "--offset",
                offset,
                "--limit",
                chunk,
                "--salt",
                salt])
            processes.append(subprocess.Popen([
                "celldb-etl-10x",
                host,
                parsed.input,
                group,
                "--offset",
                str(offset),
                "--limit",
                str(chunk),
                "--salt",
                str(salt)]))

        # make sure our last process clears out the buffer
        # beware off-by-one errors....
        processes.append(subprocess.Popen([
                "celldb-etl-10x",
                host,
                parsed.input,
                group,
                "--offset",
                str(chunk * (subprocesses - 1)),
                "--limit",
                "-1",
                "--salt",
                str(salt)]))
    else:
        if limit == -1:
            barcodes = hfile[group + "/barcodes"]
            limit_string = str(len(barcodes))
        else:
            limit_string = str(offset + limit)
        print("Attempting to upsert {} to {} from {} of {} to {}.".format(
            offset, limit_string, group, parsed.input, host))
        print("Extracting features list, this may take a moment...")
        features = get_features(hfile, group)
        featureIds = [x[0] for x in features]
        print("Done extracting {} features.".format(len(features)))
        rows = extract(hfile, group, offset, limit)
        cursor = client.connect(host).cursor()
        first = False
        for row in rows:
            sampleId, _, values = transform(row, salt)
            if first:
                # We'll load the Features table once, since each row will
                # be upserting the same values to that table.
                print("Populating features table and adding a first sample.")
                print(sampleId)
                client.upsert_sample(cursor, sampleId, featureIds, values)
                first = False
            else:
                print(load(cursor, sampleId, featureIds, values))